import streamlit as st
import pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score
)
from scipy.stats import pearsonr, spearmanr
import numpy as np
import os

st.set_page_config(page_title="Semantic Text Similarity HF", layout="wide")
st.title("Semantic Text Similarity —Å HuggingFace üåê")

# -------------------------
# –ú–æ–¥–µ–ª–∏
# -------------------------
models_available = ["BERT", "RoBERTa", "MiniLM"]

@st.cache_resource
def load_model(name):
    if name == "BERT":
        return SentenceTransformer('bert-base-nli-mean-tokens')
    elif name == "RoBERTa":
        return SentenceTransformer('roberta-base-nli-stsb-mean-tokens')
    else:
        return SentenceTransformer('all-MiniLM-L6-v2')

# -------------------------
# –í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ HuggingFace
# -------------------------
st.subheader("–í—ã–±–µ—Ä–∏—Ç–µ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç")
dataset_choice = st.selectbox("–î–∞—Ç–∞—Å–µ—Ç:", ["STS Benchmark", "Quora Question Pairs (QQP)"])
split_choice = st.selectbox("Split:", ["train", "validation", "test"])

if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç"):
    if dataset_choice == "STS Benchmark":
        dataset = load_dataset("glue", "stsb", split=split_choice)
        df = pd.DataFrame(dataset)
        df.rename(columns={"sentence1":"sentence1","sentence2":"sentence2","label":"score"}, inplace=True)
    elif dataset_choice == "Quora Question Pairs (QQP)":
        dataset = load_dataset("glue", "qqp", split=split_choice)
        df = pd.DataFrame(dataset)
        df.rename(columns={"question1":"sentence1","question2":"sentence2","label":"label"}, inplace=True)

    st.success(f"{dataset_choice} ({split_choice}) –∑–∞–≥—Ä—É–∂–µ–Ω! –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
    st.dataframe(df.head(10))

    models_hf = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏:", models_available, default=models_available)

    if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç"):
        st.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞... ‚è≥")
        results_df = df.copy()

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name in models_hf:
            model = load_model(model_name)
            sims = []
            for s1, s2 in zip(df["sentence1"], df["sentence2"]):
                emb1 = model.encode(s1, convert_to_tensor=True)
                emb2 = model.encode(s2, convert_to_tensor=True)
                sims.append(float(util.cos_sim(emb1, emb2)))
            results_df[f"{model_name}_similarity"] = sims

        st.success("–ì–æ—Ç–æ–≤–æ! –°—Ö–æ–¥—Å—Ç–≤–æ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫.")
        st.dataframe(results_df.head(10))

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not os.path.exists("data"):
            os.makedirs("data")
        results_df.to_csv("data/results.csv", index=False)
        st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/results.csv")

        # -------------------------
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        # -------------------------
        if "score" in df.columns:
            st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)")
            for model_name in models_hf:
                y_true = df["score"]
                y_pred = results_df[f"{model_name}_similarity"]
                st.write(f"**{model_name}**:")
                st.write(f"- MSE: {mean_squared_error(y_true, y_pred):.3f}")
                st.write(f"- RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.3f}")
                st.write(f"- MAE: {mean_absolute_error(y_true, y_pred):.3f}")
                st.write(f"- R¬≤: {r2_score(y_true, y_pred):.3f}")
                st.write(f"- Pearson: {pearsonr(y_true, y_pred)[0]:.3f}")
                st.write(f"- Spearman: {spearmanr(y_true, y_pred)[0]:.3f}")

        # -------------------------
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        # -------------------------
        if "label" in df.columns:
            st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)")
            for model_name in models_hf:
                y_true = df["label"]
                # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–∫—Ä—É–≥–ª—è–µ–º cosine similarity –∫ 0 –∏–ª–∏ 1
                y_pred = np.round(results_df[f"{model_name}_similarity"].values).astype(int)
                st.write(f"**{model_name}**:")
                st.write(f"- Accuracy: {accuracy_score(y_true, y_pred):.3f}")
                st.write(f"- Precision: {precision_score(y_true, y_pred, zero_division=0):.3f}")
                st.write(f"- Recall: {recall_score(y_true, y_pred, zero_division=0):.3f}")
                st.write(f"- F1-score: {f1_score(y_true, y_pred, zero_division=0):.3f}")




