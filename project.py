import streamlit as st
import pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
import numpy as np
from scipy.stats import pearsonr, spearmanr
import os

st.set_page_config(page_title="Semantic Text Similarity", layout="wide")
st.title("Semantic Text Similarity üåê")
st.write("–°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: —Ä—É—á–Ω–æ–π –≤–≤–æ–¥, CSV –∏–ª–∏ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç—ã.")

# -------------------------
# –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
# -------------------------
models_available = ["BERT", "RoBERTa", "MiniLM", "MiniLM (Multilingual)", "RuSBERT (RU)"]

@st.cache_resource
def load_model(name):
    if name == "BERT":
        return SentenceTransformer('bert-base-nli-mean-tokens')
    elif name == "RoBERTa":
        return SentenceTransformer('roberta-base-nli-stsb-mean-tokens')
    elif name == "MiniLM":
        return SentenceTransformer('all-MiniLM-L6-v2')
    elif name == "MiniLM (Multilingual)":
        return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    else:
        return SentenceTransformer('DeepPavlov/rubert-base-cased-sentence')

# -------------------------
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
# -------------------------
def compute_similarity(model, s1, s2):
    emb1 = model.encode(s1, convert_to_tensor=True)
    emb2 = model.encode(s2, convert_to_tensor=True)
    return float(util.cos_sim(emb1, emb2))

def compute_similarity_batch(model, df, col1="sentence1", col2="sentence2", batch_size=32):
    emb1 = model.encode(df[col1].tolist(), batch_size=batch_size, convert_to_tensor=True)
    emb2 = model.encode(df[col2].tolist(), batch_size=batch_size, convert_to_tensor=True)
    sims = util.cos_sim(emb1, emb2).diagonal().cpu().numpy()
    return sims

def compute_metrics(df, sim_col):
    pear, _ = pearsonr(df["score"], df[sim_col])
    spear, _ = spearmanr(df["score"], df[sim_col])
    mse = np.mean((df["score"] - df[sim_col])**2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(df["score"] - df[sim_col]))
    r2 = 1 - (np.sum((df["score"] - df[sim_col])**2) / np.sum((df["score"] - np.mean(df["score"]))**2))
    return {"Pearson": pear, "Spearman": spear, "MSE": mse, "RMSE": rmse, "MAE": mae, "R2": r2}

# -------------------------
# 1Ô∏è‚É£ –í–≤–æ–¥ –≤—Ä—É—á–Ω—É—é
# -------------------------
st.subheader("1Ô∏è‚É£ –í–≤–æ–¥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤—Ä—É—á–Ω—É—é")
sent1 = st.text_area("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1", "")
sent2 = st.text_area("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2", "")
models_manual = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞:", models_available, default=models_available, key="manual_models")

if st.button("–°—Ä–∞–≤–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é"):
    if not sent1.strip() or not sent2.strip():
        st.warning("–í–≤–µ–¥–∏—Ç–µ –æ–±–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è!")
    elif not models_manual:
        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")
    else:
        results = {}
        for model_name in models_manual:
            model = load_model(model_name)
            sim = compute_similarity(model, sent1, sent2)
            results[model_name] = sim

        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ö–æ–¥—Å—Ç–≤–∞:")
        for name, sim in results.items():
            st.write(f"**{name}**: {sim:.3f}")
            if sim > 0.8:
                st.success("–û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏")
            elif sim > 0.5:
                st.info("–ß–∞—Å—Ç–∏—á–Ω–æ –ø–æ—Ö–æ–∂–∏")
            else:
                st.warning("–†–∞–∑–Ω—ã–µ –ø–æ —Å–º—ã—Å–ª—É")
        st.bar_chart(results)

# -------------------------
# 2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ CSV
# -------------------------
st.subheader("2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ CSV")
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ CSV —Ñ–∞–π–ª", type="csv", key="csv_uploader")

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä:")
    st.dataframe(df.head())

    models_csv = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è CSV:", models_available, default=models_available, key="csv_models")

    if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è CSV"):
        if not all(col in df.columns for col in ["sentence1", "sentence2"]):
            st.error("CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'sentence1' –∏ 'sentence2'")
        else:
            results_df = df.copy()
            for model_name in models_csv:
                st.info(f"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è {model_name}... ‚è≥")
                model = load_model(model_name)
                sims = compute_similarity_batch(model, df)
                results_df[f"{model_name}_similarity"] = sims

            st.success("–ì–æ—Ç–æ–≤–æ!")
            st.dataframe(results_df.head())

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            os.makedirs("data", exist_ok=True)
            results_df.to_csv("data/results.csv", index=False)
            st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/results.csv")
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            if "score" in df.columns:
                metrics_list = []
                for model_name in models_csv:
                    metrics = compute_metrics(df, f"{model_name}_similarity")
                    metrics["Model"] = model_name
                    metrics_list.append(metrics)
                metrics_df = pd.DataFrame(metrics_list)
                st.dataframe(metrics_df)
                st.bar_chart(metrics_df.set_index("Model"))

# -------------------------
# 3Ô∏è‚É£ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç—ã
# -------------------------
st.subheader("3Ô∏è‚É£ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç—ã")
dataset_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç:", ["STS Benchmark", "Quora Question Pairs (QQP)", "RuSTS (RU)"], key="dataset_choice")

if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"):
    if dataset_choice == "STS Benchmark":
        data = load_dataset("stsb_multi_mt", name="en")
        df = data["test"].to_pandas()
        df.rename(columns={"similarity_score": "score"}, inplace=True)
    elif dataset_choice == "Quora Question Pairs (QQP)":
        data = load_dataset("glue", "qqp")
        df = data["validation"].to_pandas()
        df.rename(columns={"question1": "sentence1", "question2": "sentence2", "label": "score"}, inplace=True)
    else:  # RuSTS
        data = load_dataset("ai-forever/ru-sts")
        df = pd.DataFrame(data["test"])
        df.rename(columns={"sentence1": "sentence1", "sentence2": "sentence2", "similarity": "score"}, inplace=True)

    st.success(f"{dataset_choice} –∑–∞–≥—Ä—É–∂–µ–Ω!")
    st.dataframe(df.head())

    models_hf = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏:", models_available, default=models_available, key="hf_models")

    if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç"):
        results_df = df.copy()
        for model_name in models_hf:
            st.info(f"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è {model_name}... ‚è≥")
            model = load_model(model_name)
            sims = compute_similarity_batch(model, df)
            results_df[f"{model_name}_similarity"] = sims

        st.success("–ì–æ—Ç–æ–≤–æ!")
        st.dataframe(results_df.head())

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        os.makedirs("data", exist_ok=True)
        results_df.to_csv("data/results.csv", index=False)
        st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/results.csv")

        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics_list = []
        if "score" in df.columns:
            for model_name in models_hf:
                metrics = compute_metrics(df, f"{model_name}_similarity")
                metrics["Model"] = model_name
                metrics_list.append(metrics)
            metrics_df = pd.DataFrame(metrics_list)
            st.dataframe(metrics_df)
            st.bar_chart(metrics_df.set_index("Model"))
