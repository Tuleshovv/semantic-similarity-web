import streamlit as st
import pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score
from scipy.stats import pearsonr, spearmanr
import numpy as np
import os

st.set_page_config(page_title="Semantic Text Similarity", layout="wide")
st.title("Semantic Text Similarity üåê")
st.write("–°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–æ—Ç–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (STS, QQP) –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ —Å–≤–æ–∏ CSV.")

# -------------------------
# –ú–æ–¥–µ–ª–∏
# -------------------------
models_available = ["BERT", "RoBERTa", "MiniLM"]

@st.cache_resource
def load_model(name):
    if name == "BERT":
        return SentenceTransformer('bert-base-nli-mean-tokens')
    elif name == "RoBERTa":
        return SentenceTransformer('roberta-base-nli-stsb-mean-tokens')
    else:
        return SentenceTransformer('all-MiniLM-L6-v2')

# ==========================================================
# 1) –í–≤–æ–¥ –≤—Ä—É—á–Ω—É—é
# ==========================================================
st.subheader("–í–≤–æ–¥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤—Ä—É—á–Ω—É—é")
sent1 = st.text_area("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1", "")
sent2 = st.text_area("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2", "")
models_manual = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏:", models_available, default=models_available, key="manual_models")

if st.button("–°—Ä–∞–≤–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é"):
    if sent1.strip() == "" or sent2.strip() == "":
        st.warning("–í–≤–µ–¥–∏—Ç–µ –æ–±–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è!")
    elif not models_manual:
        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å!")
    else:
        results = {}
        for model_name in models_manual:
            model = load_model(model_name)
            emb1 = model.encode(sent1, convert_to_tensor=True)
            emb2 = model.encode(sent2, convert_to_tensor=True)
            similarity = float(util.cos_sim(emb1, emb2))
            results[model_name] = similarity

        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ö–æ–¥—Å—Ç–≤–∞:")
        for name, sim in results.items():
            st.write(f"**{name}**: {sim:.3f}")
            if sim > 0.8:
                st.success("–û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏")
            elif sim > 0.5:
                st.info("–ß–∞—Å—Ç–∏—á–Ω–æ –ø–æ—Ö–æ–∂–∏")
            else:
                st.warning("–†–∞–∑–Ω—ã–µ –ø–æ —Å–º—ã—Å–ª—É")

        st.bar_chart(results)

# ==========================================================
# 2) –ó–∞–≥—Ä—É–∑–∫–∞ CSV
# ==========================================================
st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (CSV)")
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ CSV —Ñ–∞–π–ª", type="csv", key="csv_uploader")

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä:")
    st.dataframe(df.head())

    models_csv = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è CSV:", models_available, default=models_available, key="csv_models")

    if st.button("–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è CSV"):
        if not all(col in df.columns for col in ["sentence1", "sentence2"]):
            st.error("CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'sentence1' –∏ 'sentence2'")
        else:
            results_df = df.copy()
            st.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞... ‚è≥")

            for model_name in models_csv:
                model = load_model(model_name)
                sims = []
                for s1, s2 in zip(df["sentence1"], df["sentence2"]):
                    emb1 = model.encode(s1, convert_to_tensor=True)
                    emb2 = model.encode(s2, convert_to_tensor=True)
                    sims.append(float(util.cos_sim(emb1, emb2)))
                results_df[f"{model_name}_similarity"] = sims

            st.success("–ì–æ—Ç–æ–≤–æ!")
            st.dataframe(results_df.head())

            if not os.path.exists("data"):
                os.makedirs("data")
            results_df.to_csv("data/results.csv", index=False)
            st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/results.csv")

            # –ú–µ—Ç—Ä–∏–∫–∏
            if "score" in df.columns:
                st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)")
                metrics_list = []
                for model_name in models_csv:
                    mse = mean_squared_error(df["score"], results_df[f"{model_name}_similarity"])
                    rmse = np.sqrt(mse)
                    pear, _ = pearsonr(df["score"], results_df[f"{model_name}_similarity"])
                    spear, _ = spearmanr(df["score"], results_df[f"{model_name}_similarity"])
                    metrics_list.append({
                        "Model": model_name,
                        "MSE": mse,
                        "RMSE": rmse,
                        "Pearson": pear,
                        "Spearman": spear
                    })
                    st.write(f"**{model_name}** ‚Äî MSE: {mse:.3f}, RMSE: {rmse:.3f}, Pearson: {pear:.3f}, Spearman: {spear:.3f}")
                st.bar_chart(pd.DataFrame(metrics_list).set_index("Model"))

            if "label" in df.columns:
                st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)")
                for model_name in models_csv:
                    # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–∫—Ä—É–≥–ª—è–µ–º similarity –∫ 0/1
                    pred = np.round(results_df[f"{model_name}_similarity"].values)
                    accuracy = accuracy_score(df["label"], pred)
                    precision = precision_score(df["label"], pred)
                    recall = recall_score(df["label"], pred)
                    f1 = f1_score(df["label"], pred)
                    st.write(f"**{model_name}** ‚Äî Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}")

# ==========================================================
# 3) HuggingFace –¥–∞—Ç–∞—Å–µ—Ç—ã
# ==========================================================
st.subheader("–ì–æ—Ç–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (HuggingFace)")
dataset_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç:", ["STS Benchmark", "Quora Question Pairs (QQP)"], key="dataset_choice")

if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"):
    if dataset_choice == "STS Benchmark":
        data = load_dataset("stsb_multi_mt", name="en")
        df = data["test"].to_pandas()
        df.rename(columns={"similarity_score": "score"}, inplace=True)
    elif dataset_choice == "Quora Question Pairs (QQP)":
        data = load_dataset("glue", "qqp")
        df = data["validation"].to_pandas()
        df.rename(columns={
            "question1": "sentence1",
            "question2": "sentence2",
            "label": "label"
        }, inplace=True)

    st.success(f"{dataset_choice} –∑–∞–≥—Ä—É–∂–µ–Ω!")
    st.dataframe(df.head())

    models_hf = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏:", models_available, default=models_available, key="hf_models")

    if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç HuggingFace"):
        results_df = df.copy()
        st.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞... ‚è≥")

        for model_name in models_hf:
            model = load_model(model_name)
            sims = []
            for s1, s2 in zip(df["sentence1"], df["sentence2"]):
                emb1 = model.encode(s1, convert_to_tensor=True)
                emb2 = model.encode(s2, convert_to_tensor=True)
                sims.append(float(util.cos_sim(emb1, emb2)))
            results_df[f"{model_name}_similarity"] = sims

        st.success("–ì–æ—Ç–æ–≤–æ!")
        st.dataframe(results_df.head())

        if not os.path.exists("data"):
            os.makedirs("data")
        results_df.to_csv("data/results.csv", index=False)
        st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/results.csv")

        # –ú–µ—Ç—Ä–∏–∫–∏
        if "score" in df.columns:
            st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)")
            metrics_list = []
            for model_name in models_hf:
                mse = mean_squared_error(df["score"], results_df[f"{model_name}_similarity"])
                rmse = np.sqrt(mse)
                pear, _ = pearsonr(df["score"], results_df[f"{model_name}_similarity"])
                spear, _ = spearmanr(df["score"], results_df[f"{model_name}_similarity"])
                metrics_list.append({
                    "Model": model_name,
                    "MSE": mse,
                    "RMSE": rmse,
                    "Pearson": pear,
                    "Spearman": spear
                })
                st.write(f"**{model_name}** ‚Äî MSE: {mse:.3f}, RMSE: {rmse:.3f}, Pearson: {pear:.3f}, Spearman: {spear:.3f}")
            st.bar_chart(pd.DataFrame(metrics_list).set_index("Model"))

        if "label" in df.columns:
            st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)")
            for model_name in models_hf:
                pred = np.round(results_df[f"{model_name}_similarity"].values)
                accuracy = accuracy_score(df["label"], pred)
                precision = precision_score(df["label"], pred)
                recall = recall_score(df["label"], pred)
                f1 = f1_score(df["label"], pred)
                st.write(f"**{model_name}** ‚Äî Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}")


